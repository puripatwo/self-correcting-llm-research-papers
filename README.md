# Self-Correcting LLMs Research Papers 2024

## 1. Comics Generation
### 1.1. Stylized Art

### 1.2. Previous Work

## 2. Generative Models
### 2.1. GANs
[1] **Generative Adversarial Networks** [paper](https://arxiv.org/pdf/1406.2661)

[2] Large Scale GAN Training for High Fidelity Natural Image Synthesis [paper](https://arxiv.org/abs/1809.11096)

[3] A Style-Based Generator Architecture for Generative Adversarial Networks [paper](https://arxiv.org/pdf/1812.04948)

[4] Analyzing and Improving the Image Quality of StyleGAN [paper](https://arxiv.org/abs/1912.04958)

[5] StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation [paper](https://arxiv.org/abs/2011.12799)

### 2.2. Diffusion Models
[1] Deep Unsupervised Learning using Nonequilibrium Thermodynamics [paper](https://arxiv.org/abs/1503.03585)

[2] **Denoising Diffusion Probabilistic Models** [paper](https://arxiv.org/pdf/2006.11239)

[3] Improved Denoising Diffusion Probabilistic Models [paper](https://arxiv.org/abs/2102.09672)

[4] Elucidating the Design Space of Diffusion-Based Generative Models [paper](https://arxiv.org/abs/2206.00364)

[5] Diffusion Models Beat GANs on Image Synthesis [paper](https://arxiv.org/abs/2105.05233)

[6] Classifier-Free Diffusion Guidance [paper](https://arxiv.org/abs/2207.12598)

[7] Cascaded Diffusion Models for High Fidelity Image Generation [paper](https://arxiv.org/abs/2106.15282)

[8] Score-Based Generative Modeling through Stochastic Differential Equations [paper](https://arxiv.org/abs/2011.13456)

[9] Learning Transferable Visual Models From Natural Language Supervision [paper](https://arxiv.org/abs/2103.00020)

[10] Understanding Diffusion Models: A Unified Perspective [paper](https://arxiv.org/abs/2208.11970)

## 3. Text-to-Image Generative Models
### 3.1. Text-to-Image Diffusion Models
[1] Zero-Shot Text-to-Image Generation [paper](https://arxiv.org/abs/2102.12092)

[2] Hierarchical Text-Conditional Image Generation with CLIP Latents [paper](https://arxiv.org/abs/2204.06125)

[3] High-Resolution Image Synthesis with Latent Diffusion Models [paper](https://arxiv.org/pdf/2112.10752)

[4] Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding [paper](https://arxiv.org/abs/2205.11487)

[5] Training-Free Layout Control with Cross-Attention Guidance [paper](https://arxiv.org/pdf/2304.03373)

[6] GLIGEN: Open-Set Grounded Text-to-Image Generation [paper](https://arxiv.org/pdf/2301.07093)

[7] Localizing Object-level Shape Variations with Text-to-Image Diffusion Models [paper](https://arxiv.org/pdf/2303.11306)

[8] Paint by Example: Exemplar-based Image Editing with Diffusion Models [paper](https://arxiv.org/pdf/2211.13227)

[9] ReCo: Region-Controlled Text-to-Image Generation [paper](https://arxiv.org/pdf/2211.15518)

[10] MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation [paper](https://arxiv.org/abs/2302.08113)

[11] BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion [paper](https://arxiv.org/abs/2307.10816)

[12] SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis [paper](https://arxiv.org/pdf/2307.01952)

[13] Improving Image Generation with Better Captions [paper](https://cdn.openai.com/papers/dall-e-3.pdf)

[14] PixArt-Î±: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis [paper](https://arxiv.org/abs/2310.00426)

[15] Scalable Diffusion Models with Transformers [paper](https://arxiv.org/abs/2212.09748)

[16] BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing [paper](https://arxiv.org/abs/2305.14720)

[17] TextDiffuser: Diffusion Models as Text Painters [paper](https://arxiv.org/abs/2305.10855)

[18] TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering [paper](https://arxiv.org/abs/2311.16465)

[19] T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation [paper](https://arxiv.org/abs/2307.06350)

[20] Scaling Autoregressive Models for Content-Rich Text-to-Image Generation [paper](https://arxiv.org/abs/2206.10789)

### 3.2. Large Language Models
[1] **Attention Is All You Need** [paper](https://arxiv.org/pdf/1706.03762)

[2] LayoutGPT: Compositional Visual Planning and Generation with Large Language Models [paper](https://arxiv.org/pdf/2305.15393)

[3] LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models [paper](https://arxiv.org/pdf/2305.13655)

[4] LLM-GROUNDED VIDEO DIFFUSION MODELS [paper](https://arxiv.org/pdf/2309.17444)

[5] VIDEODIRECTORGPT: Consistent Multi-Scene Video Generation via LLM-Guided Planning [paper](https://arxiv.org/pdf/2309.15091)

[6] Controllable Text-to-Image Generation with GPT-4 [paper](https://arxiv.org/pdf/2305.18583)

[7] DiffusionGPT: LLM-Driven Text-to-Image Generation System [paper](https://arxiv.org/abs/2401.10061)

## 4. Self-Correcting Text-to-Image Generative Models
### 4.1. Self-Correcting LLMs
[1] **Self-Correcting LLM-controlled Diffusion Models** [paper](https://arxiv.org/pdf/2311.16090)

[2] Self-Consuming Models Go MAD [paper](https://arxiv.org/pdf/2307.01850)
 
[3] Self-Correcting Self-Consuming Loops for Generative Model Training [paper](https://arxiv.org/pdf/2402.07087)
 
[4] GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing [paper](https://arxiv.org/pdf/2407.05600)
 
[5] GG-Editor: Locally Editing 3D Avatars with Multimodal Large Language Model Guidance [paper](https://openreview.net/pdf?id=31rrsYnriG)

[6] Concept Sliders [paper](https://arxiv.org/pdf/2311.12092)

[7] **Prompt Sliders for Fine-Grained Control, Editing and Erasing of Concepts in Diffusion Models** [paper](https://arxiv.org/pdf/2409.16535)

[8] Adding Conditional Control to Text-to-Image Diffusion Models [paper](https://arxiv.org/pdf/2302.05543)

[9] Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects [paper](https://arxiv.org/pdf/2409.02653)

[10] A Survey on Self-Evolution of Large Language Models [paper](https://arxiv.org/pdf/2404.14387)

[11] Segment Anything [paper](https://arxiv.org/abs/2304.02643)

[12] Scaling Open-Vocabulary Object Detection [paper](https://arxiv.org/abs/2306.09683)

[13] A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT [paper](https://arxiv.org/abs/2302.11382)

[14] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [paper](https://arxiv.org/abs/2201.11903)

### 4.2. Image Editing
[1] Prompt-to-Prompt Image Editing with Cross Attention Control [paper](https://arxiv.org/pdf/2208.01626)

[2] InstructPix2Pix: Learning to Follow Image Editing Instructions [paper](https://arxiv.org/abs/2211.09800)

[3] SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations [paper](https://arxiv.org/abs/2108.01073)

[4] DiffEdit: Diffusion-based semantic image editing with mask guidance [paper](https://arxiv.org/abs/2210.11427)

[5] Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation [paper](https://arxiv.org/abs/2211.12572)

[6] Diffusion Self-Guidance for Controllable Image Generation [paper](https://arxiv.org/abs/2306.00986)

[7] InstructDiffusion: A Generalist Modeling Interface for Vision Tasks [paper](https://arxiv.org/abs/2309.03895)

[8] DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation [paper](https://arxiv.org/abs/2110.02711)

[9] GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models [paper](https://arxiv.org/abs/2112.10741)

[10] DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing [paper](https://arxiv.org/abs/2306.14435)

[11] Imagic: Text-Based Real Image Editing with Diffusion Models [paper](https://arxiv.org/abs/2210.09276)

[12] Zero-shot Image-to-Image Translation [paper](https://arxiv.org/abs/2302.03027)

[13] Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry [paper](https://arxiv.org/abs/2307.12868)

[14] Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2212.08698)

[15] MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing [paper](https://arxiv.org/abs/2306.10012)

[16] Emu Edit: Precise Image Editing via Recognition and Generation Tasks [paper](https://arxiv.org/abs/2311.10089)

[17] Cones 2: Customizable Image Synthesis with Multiple Subjects [paper](https://arxiv.org/abs/2305.19327)

[18] Sketch-Guided Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2211.13752)

### 4.3 Model Editing
[1] Erasing Concepts from Diffusion Models [paper](https://arxiv.org/abs/2303.07345)

[2] Low-rank adaptation for Erasing COncepts from diffusion models [github](https://github.com/p1atdev/LECO/tree/main)

[3] SD/SDXL Tricks beneath the Papers and Codes [github](https://normxu.github.io/sd-tricks/)

[4] Unified Concept Editing in Diffusion Models [paper](https://arxiv.org/abs/2308.14761)

[5] Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2307.05977)

[6] Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models [paper](https://arxiv.org/abs/2305.10120)

[7] Ablating Concepts in Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2303.13516)

[8] Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2303.17591)

[9] Editing Implicit Assumptions in Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2303.08084)

[10] Controllable Generation with Text-to-Image Diffusion Models: A Survey [paper](https://arxiv.org/abs/2403.04279)

### 4.4 Personalization
[1] "This is my unicorn, Fluffy": Personalizing frozen vision-language representations [paper](https://arxiv.org/abs/2204.01694)

[2] MyStyle: A Personalized Generative Prior [paper](https://arxiv.org/abs/2203.17272)

[3] **An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion** [paper](https://arxiv.org/abs/2208.01618)

[4] DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation [paper](https://arxiv.org/abs/2208.12242)

[5] Multi-Concept Customization of Text-to-Image Diffusion [paper](https://arxiv.org/abs/2212.04488)

[6] LoRA: Low-Rank Adaptation of Large Language Models [paper](https://arxiv.org/pdf/2106.09685)

[7] Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning [github](https://github.com/cloneofsimo/lora)

[8] Key-Locked Rank One Editing for Text-to-Image Personalization [paper](https://arxiv.org/abs/2305.01644)

[9] SVDiff: Compact Parameter Space for Diffusion Fine-Tuning [paper](https://arxiv.org/abs/2303.11305)

[10] Break-A-Scene: Extracting Multiple Concepts from a Single Image [paper](https://arxiv.org/abs/2305.16311)

[11] Subject-driven Text-to-Image Generation via Apprenticeship Learning [paper](https://arxiv.org/abs/2304.00186)

[12] Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models [paper](https://arxiv.org/abs/2302.12228)

[13] Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2304.02642)

[14] InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning [paper](https://arxiv.org/abs/2304.03411)

[15] ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation [paper](https://arxiv.org/abs/2302.13848)

[16] P+: Extended Textual Conditioning in Text-to-Image Generation [paper](https://arxiv.org/abs/2303.09522)
