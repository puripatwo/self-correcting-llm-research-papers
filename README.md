# Self-Correcting LLMs Research Papers 2024

## 1. Generative Models
### 1.1. GANs
[1] Generative Adversarial Networks [paper](https://arxiv.org/pdf/1406.2661)

[2] Large Scale GAN Training for High Fidelity Natural Image Synthesis [paper](https://arxiv.org/abs/1809.11096)

[3] A Style-Based Generator Architecture for Generative Adversarial Networks [paper](https://arxiv.org/pdf/1812.04948)

[4] Analyzing and Improving the Image Quality of StyleGAN [paper](https://arxiv.org/abs/1912.04958)

[5] StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation [paper](https://arxiv.org/abs/2011.12799)

### 1.2. Diffusion Models
[1] Deep Unsupervised Learning using Nonequilibrium Thermodynamics [paper](https://arxiv.org/abs/1503.03585)

[2] Denoising Diffusion Probabilistic Models [paper](https://arxiv.org/pdf/2006.11239)

[3] Improved Denoising Diffusion Probabilistic Models [paper](https://arxiv.org/abs/2102.09672)

[4] Elucidating the Design Space of Diffusion-Based Generative Models [paper](https://arxiv.org/abs/2206.00364)

[5] Analyzing and Improving the Training Dynamics of Diffusion Models [paper](https://arxiv.org/abs/2312.02696)

[6] Diffusion Models Beat GANs on Image Synthesis [paper](https://arxiv.org/abs/2105.05233)

[7] Classifier-Free Diffusion Guidance [paper](https://arxiv.org/abs/2207.12598)

[8] Score-based Generative Modeling in Latent Space [paper](https://arxiv.org/abs/2106.05931)

[9] Understanding Diffusion Models: A Unified Perspective [paper](https://arxiv.org/abs/2208.11970)

[10] Tutorial on Diffusion Models for Imaging and Vision [paper](https://arxiv.org/pdf/2403.18103)

## 2. Text-to-Image Generative Models
### 2.1. Text-to-Image Diffusion Models
[1] Zero-Shot Text-to-Image Generation [paper](https://arxiv.org/abs/2102.12092)

[2] Hierarchical Text-Conditional Image Generation with CLIP Latents [paper](https://arxiv.org/abs/2204.06125)

[3] Improving Image Generation with Better Captions [paper](https://cdn.openai.com/papers/dall-e-3.pdf)

[4] High-Resolution Image Synthesis with Latent Diffusion Models [paper](https://arxiv.org/pdf/2112.10752)

[5] Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding [paper](https://arxiv.org/abs/2205.11487)

[6] SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis [paper](https://arxiv.org/pdf/2307.01952)

[7] Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors [paper](https://arxiv.org/abs/2203.13131)

[8] Scaling Autoregressive Models for Content-Rich Text-to-Image Generation [paper](https://arxiv.org/abs/2206.10789)

[9] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale [paper](https://arxiv.org/abs/2010.11929)

[10] Learning Transferable Visual Models From Natural Language Supervision [paper](https://arxiv.org/abs/2103.00020)

### 2.2. Large Language Models
[1] **Attention Is All You Need** [paper](https://arxiv.org/pdf/1706.03762)

[2] Controllable Text-to-Image Generation with GPT-4 [paper](https://arxiv.org/pdf/2305.18583)

[3] LayoutGPT: Compositional Visual Planning and Generation with Large Language Models [paper](https://arxiv.org/pdf/2305.15393)

[4] DiffusionGPT: LLM-Driven Text-to-Image Generation System [paper](https://arxiv.org/abs/2401.10061)

[5] LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models [paper](https://arxiv.org/pdf/2305.13655)

[6] LLM-GROUNDED VIDEO DIFFUSION MODELS [paper](https://arxiv.org/pdf/2309.17444)

[7] VIDEODIRECTORGPT: Consistent Multi-Scene Video Generation via LLM-Guided Planning [paper](https://arxiv.org/pdf/2309.15091)

## 3. Self-Correcting Text-to-Image Generative Models
[1] Self-Correcting LLM-controlled Diffusion Models [paper](https://arxiv.org/pdf/2311.16090)

[2] Self-Consuming Models Go MAD [paper](https://arxiv.org/pdf/2307.01850)
 
[3] Self-Correcting Self-Consuming Loops for Generative Model Training [paper](https://arxiv.org/pdf/2402.07087)
 
[4] GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing [paper](https://arxiv.org/pdf/2407.05600)
 
[5] GG-Editor: Locally Editing 3D Avatars with Multimodal Large Language Model Guidance [paper](https://openreview.net/pdf?id=31rrsYnriG)

[6] Concept Sliders [paper](https://arxiv.org/pdf/2311.12092)

[7] **Prompt Sliders for Fine-Grained Control, Editing and Erasing of Concepts in Diffusion Models** [paper](https://arxiv.org/pdf/2409.16535)

[8] Adding Conditional Control to Text-to-Image Diffusion Models [paper](https://arxiv.org/pdf/2302.05543)

[9] Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects [paper](https://arxiv.org/pdf/2409.02653)

[10] A Survey on Self-Evolution of Large Language Models [paper](https://arxiv.org/pdf/2404.14387)

[11] Segment Anything [paper](https://arxiv.org/abs/2304.02643)

[12] Scaling Open-Vocabulary Object Detection [paper](https://arxiv.org/abs/2306.09683)

[13] A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT [paper](https://arxiv.org/abs/2302.11382)

[14] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [paper](https://arxiv.org/abs/2201.11903)

## 4. Image Editing
### 4.1. Diffusion-Based Image Editing
[1] SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations [paper](https://arxiv.org/abs/2108.01073)

[2] GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models [paper](https://arxiv.org/abs/2112.10741)

[3] Prompt-to-Prompt Image Editing with Cross Attention Control [paper](https://arxiv.org/pdf/2208.01626)

[4] DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation [paper](https://arxiv.org/abs/2110.02711)

[5] DiffEdit: Diffusion-based semantic image editing with mask guidance [paper](https://arxiv.org/abs/2210.11427)

[6] Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation [paper](https://arxiv.org/abs/2211.12572)

[7] Sketch-Guided Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2211.13752)

[8] Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2212.08698)

[9] InstructPix2Pix: Learning to Follow Image Editing Instructions [paper](https://arxiv.org/abs/2211.09800)

[10] Zero-shot Image-to-Image Translation [paper](https://arxiv.org/abs/2302.03027)

[11] Imagic: Text-Based Real Image Editing with Diffusion Models [paper](https://arxiv.org/abs/2210.09276)

[12] Cones 2: Customizable Image Synthesis with Multiple Subjects [paper](https://arxiv.org/abs/2305.19327)

[13] Diffusion Self-Guidance for Controllable Image Generation [paper](https://arxiv.org/abs/2306.00986)

[14] InstructDiffusion: A Generalist Modeling Interface for Vision Tasks [paper](https://arxiv.org/abs/2309.03895)

[15] Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry [paper](https://arxiv.org/abs/2307.12868)

[16] Emu Edit: Precise Image Editing via Recognition and Generation Tasks [paper](https://arxiv.org/abs/2311.10089)

[17] DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing [paper](https://arxiv.org/abs/2306.14435)

[18] MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing [paper](https://arxiv.org/abs/2306.10012)

[19] PixArt-Î±: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis [paper](https://arxiv.org/abs/2310.00426)

[20] BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing [paper](https://arxiv.org/abs/2305.14720)

### 4.2. Layout-Controlled Generation
[1] Paint by Example: Exemplar-based Image Editing with Diffusion Models [paper](https://arxiv.org/pdf/2211.13227)

[2] ReCo: Region-Controlled Text-to-Image Generation [paper](https://arxiv.org/pdf/2211.15518)

[3] GLIGEN: Open-Set Grounded Text-to-Image Generation [paper](https://arxiv.org/pdf/2301.07093)

[4] Localizing Object-level Shape Variations with Text-to-Image Diffusion Models [paper](https://arxiv.org/pdf/2303.11306)

[5] Training-Free Layout Control with Cross-Attention Guidance [paper](https://arxiv.org/pdf/2304.03373)

[6] MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation [paper](https://arxiv.org/abs/2302.08113)

[7] BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion [paper](https://arxiv.org/abs/2307.10816)

[8] DreamFusion: Text-to-3D using 2D Diffusion [paper](https://arxiv.org/abs/2209.14988)

## 5. Model Editing
[1] Erasing Concepts from Diffusion Models [paper](https://arxiv.org/abs/2303.07345)

[2] Low-rank adaptation for Erasing COncepts from diffusion models [github](https://github.com/p1atdev/LECO/tree/main)

[3] SD/SDXL Tricks beneath the Papers and Codes [github](https://normxu.github.io/sd-tricks/)

[4] Unified Concept Editing in Diffusion Models [paper](https://arxiv.org/abs/2308.14761)

[5] Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2307.05977)

[6] Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models [paper](https://arxiv.org/abs/2305.10120)

[7] Ablating Concepts in Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2303.13516)

[8] Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2303.17591)

[9] Editing Implicit Assumptions in Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2303.08084)

[10] Controllable Generation with Text-to-Image Diffusion Models: A Survey [paper](https://arxiv.org/abs/2403.04279)

## 6. Personalization
[1] "This is my unicorn, Fluffy": Personalizing frozen vision-language representations [paper](https://arxiv.org/abs/2204.01694)

[2] MyStyle: A Personalized Generative Prior [paper](https://arxiv.org/abs/2203.17272)

[3] An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion [paper](https://arxiv.org/abs/2208.01618)

[4] DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation [paper](https://arxiv.org/abs/2208.12242)

[5] Multi-Concept Customization of Text-to-Image Diffusion [paper](https://arxiv.org/abs/2212.04488)

[6] LoRA: Low-Rank Adaptation of Large Language Models [paper](https://arxiv.org/pdf/2106.09685)

[7] Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning [github](https://github.com/cloneofsimo/lora)

[8] Key-Locked Rank One Editing for Text-to-Image Personalization [paper](https://arxiv.org/abs/2305.01644)

[9] SVDiff: Compact Parameter Space for Diffusion Fine-Tuning [paper](https://arxiv.org/abs/2303.11305)

[10] Break-A-Scene: Extracting Multiple Concepts from a Single Image [paper](https://arxiv.org/abs/2305.16311)

[11] Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models [paper](https://arxiv.org/abs/2302.12228)

[12] Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models [paper](https://arxiv.org/abs/2304.02642)

[13] InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning [paper](https://arxiv.org/abs/2304.03411)

[14] ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation [paper](https://arxiv.org/abs/2302.13848)

[15] Subject-driven Text-to-Image Generation via Apprenticeship Learning [paper](https://arxiv.org/abs/2304.00186)

[16] P+: Extended Textual Conditioning in Text-to-Image Generation [paper](https://arxiv.org/abs/2303.09522)
